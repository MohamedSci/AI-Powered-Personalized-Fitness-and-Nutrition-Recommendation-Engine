{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b4z2ieyaB9yB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install torch\n",
        "!pip install skl2onnx\n",
        "!pip install tf2onnx"
      ],
      "metadata": {
        "id": "qF3JEvrrB-6w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a05344b5-faa7-4ca1-d239-3cb3c3631feb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "c07aac29213c4df29fe044c371da0a58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting skl2onnx\n",
            "  Downloading skl2onnx-1.18.0-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting onnx>=1.2.1 (from skl2onnx)\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.1 in /usr/local/lib/python3.11/dist-packages (from skl2onnx) (1.6.1)\n",
            "Collecting onnxconverter-common>=1.7.0 (from skl2onnx)\n",
            "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.2.1->skl2onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.2.1->skl2onnx) (5.29.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxconverter-common>=1.7.0->skl2onnx) (24.2)\n",
            "Collecting protobuf>=3.20.2 (from onnx>=1.2.1->skl2onnx)\n",
            "  Downloading protobuf-3.20.2-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->skl2onnx) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->skl2onnx) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1->skl2onnx) (3.6.0)\n",
            "Downloading skl2onnx-1.18.0-py2.py3-none-any.whl (300 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.2-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, onnx, onnxconverter-common, skl2onnx\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.16.1 requires protobuf<6.0.0dev,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.2 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.17.0 onnxconverter-common-1.14.0 protobuf-3.20.2 skl2onnx-1.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "bec4d172ec5c48aa807c0b7db78e0db5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.16.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (2.0.2)\n",
            "Requirement already satisfied: onnx>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (1.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (1.17.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (25.2.10)\n",
            "Requirement already satisfied: protobuf~=3.20 in /usr/local/lib/python3.11/dist-packages (from tf2onnx) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->tf2onnx) (2025.1.31)\n",
            "Downloading tf2onnx-1.16.1-py3-none-any.whl (455 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m455.8/455.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tf2onnx\n",
            "Successfully installed tf2onnx-1.16.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IACf4HtHI8xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x"
      ],
      "metadata": {
        "id": "b59YDIDcKUCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97b802e4-adb4-4ded-bbe9-8ad47e85c86c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "UYPBKJi-KcVX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ac6917-798e-46db-ee4d-6213848359c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqKp2400Bp8r",
        "outputId": "38edce0d-cb6a-466b-a744-a7dcce47bcfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:TensorFlow version 2.18.0 might be older than recommended for ONNX to TFLite conversion. Consider upgrading to 2.7 or higher.\n",
            "ERROR:root:Error converting /content/drive/MyDrive/Projects/Get_Fit_App/Ai_Model/models/fitness_level_model.pkl to TensorFlow Lite: type object 'TFLiteConverterV2' has no attribute 'from_onnx'\n",
            "ERROR:root:Error converting /content/drive/MyDrive/Projects/Get_Fit_App/Ai_Model/models/training_params_model.pkl to TensorFlow Lite: type object 'TFLiteConverterV2' has no attribute 'from_onnx'\n",
            "ERROR:root:Error converting /content/drive/MyDrive/Projects/Get_Fit_App/Ai_Model/models/dietary_needs_model.pkl to TensorFlow Lite: type object 'TFLiteConverterV2' has no attribute 'from_onnx'\n",
            "ERROR:root:Error loading exercise database: \"['name'] not found in axis\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted Fitness Level: Intermediate\n",
            "\n",
            "--- Predicted Training Plan ---\n",
            "{'workout_frequency': '3 days per week', 'exercises': [{'name': 'Barbell Squat', 'sets': 3, 'repetitions': 8, 'details': None, 'muscle_group': None}, {'name': 'Bench Press', 'sets': 3, 'repetitions': 8, 'details': None, 'muscle_group': None}, {'name': 'Deadlift', 'sets': 10, 'repetitions': 11, 'details': None, 'muscle_group': None}, {'name': 'Overhead Press', 'sets': 12, 'repetitions': 13, 'details': None, 'muscle_group': None}]}\n",
            "\n",
            "--- Predicted Dietary Needs ---\n",
            "{'daily_calories': 3069, 'macronutrient_targets': {'protein': 153, 'carbs': 307, 'fat': 102}}\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "AI-Powered Personalized Fitness and Nutrition Recommendation Engine - Production-Ready Code for Google Colab\n",
        "(Generates Models Suitable for Flutter App via TensorFlow Lite Conversion)\n",
        "\n",
        "This script implements a comprehensive AI model for generating personalized fitness and\n",
        "nutrition plans and includes functionality to convert the trained scikit-learn models\n",
        "to TensorFlow Lite format for use in Flutter applications.\n",
        "\n",
        "Author: Mohamed Said Ibrahim (Refactored and Enhanced by Gemini)\n",
        "Date: April 1, 2025\n",
        "Version: 1.4 (Refactored, Enhanced, and Flutter Ready)\n",
        "\"\"\"\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, r2_score\n",
        "import joblib\n",
        "import logging\n",
        "import os\n",
        "from typing import Dict, List, Union, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "import tensorflow as tf\n",
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import FloatTensorType\n",
        "\n",
        "# --- 2. Configuration Management ---\n",
        "@dataclass\n",
        "class Configuration:\n",
        "    \"\"\"Configuration class to manage file paths, hyperparameters, and model settings.\"\"\"\n",
        "    DRIVE_ROOT: str = '/content/drive/MyDrive'\n",
        "    PROJECT_FOLDER: str = 'Projects/Get_Fit_App/Ai_Model'\n",
        "    DATA_FOLDER: str = 'Data_Source'\n",
        "    DATA_DIR: str = os.path.join(DRIVE_ROOT, PROJECT_FOLDER, DATA_FOLDER)\n",
        "    MODEL_DIR: str = os.path.join(DRIVE_ROOT, PROJECT_FOLDER, 'models')  # Folder for original scikit-learn models\n",
        "    TFLITE_MODEL_DIR: str = os.path.join(DRIVE_ROOT, PROJECT_FOLDER, 'tflite_models')  # Folder for TensorFlow Lite models\n",
        "    FITNESS_LEVEL_DATA_FILE: str = 'fitness_level_data_example.csv'\n",
        "    TRAINING_PARAMS_DATA_FILE: str = 'training_params_data_example.csv'\n",
        "    DIETARY_NEEDS_DATA_FILE: str = 'dietary_needs_data_example.csv'\n",
        "    EXERCISE_DATABASE_FILE: str = 'exercise_database_example.csv'\n",
        "    FITNESS_LEVEL_MODEL_NAME: str = 'fitness_level_model.pkl'\n",
        "    TRAINING_PARAMS_MODEL_NAME: str = 'training_params_model.pkl'\n",
        "    DIETARY_NEEDS_MODEL_NAME: str = 'dietary_needs_model.pkl'\n",
        "    N_SPLITS_CV: int = 5\n",
        "    RANDOM_STATE: int = 42\n",
        "    LOG_LEVEL: int = logging.INFO\n",
        "    FITNESS_LEVEL_MODEL_TYPE: str = 'random_forest'  # Options: 'random_forest', 'linear_regression' (for testing)\n",
        "    TRAINING_PARAMS_MODEL_TYPE: str = 'random_forest' # Options: 'random_forest', 'linear_regression'\n",
        "    DIETARY_NEEDS_MODEL_TYPE: str = 'linear_regression' # Options: 'linear_regression', 'random_forest'\n",
        "    TRAINING_PLAN_CONFIG: Dict = field(default_factory=lambda: {\n",
        "        \"workout_frequency\": \"3 days per week\",\n",
        "        \"exercises\": [\n",
        "            {\"name\": \"Barbell Squat\", \"sets_index\": 0, \"reps_index\": 1},\n",
        "            {\"name\": \"Bench Press\", \"sets_index\": 2, \"reps_index\": 3},\n",
        "            {\"name\": \"Deadlift\", \"sets_index\": 4, \"reps_index\": 5},\n",
        "            {\"name\": \"Overhead Press\", \"sets_index\": 6, \"reps_index\": 7}\n",
        "            # Add more exercises and their corresponding prediction indices based on your model output\n",
        "        ]\n",
        "    })\n",
        "\n",
        "# Initialize configuration and logging\n",
        "config = Configuration()\n",
        "os.makedirs(config.MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(config.TFLITE_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "logging.basicConfig(level=config.LOG_LEVEL, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')\n",
        "\n",
        "# --- 3. Data Loading and Preprocessing Module ---\n",
        "class DataPreprocessor:\n",
        "    \"\"\"Handles loading, validation, and preprocessing of data.\"\"\"\n",
        "    def load_data(self, file_path: str, expected_columns: List[str] = None) -> pd.DataFrame:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            if df.empty:\n",
        "                logging.warning(f\"Loaded data from {file_path} is empty.\")\n",
        "            if expected_columns:\n",
        "                missing_columns = [col for col in expected_columns if col not in df.columns]\n",
        "                if missing_columns:\n",
        "                    logging.error(f\"Missing expected columns in {file_path}: {missing_columns}\")\n",
        "                    raise ValueError(f\"Missing columns: {missing_columns}\")\n",
        "                logging.info(f\"Data loaded from {file_path} with expected columns.\")\n",
        "            else:\n",
        "                logging.info(f\"Data loaded from {file_path}.\")\n",
        "            return df\n",
        "        except FileNotFoundError:\n",
        "            logging.error(f\"Data file not found at: {file_path}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading data from {file_path}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def preprocess_user_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        logging.info(\"Preprocessing user data...\")\n",
        "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "        for col in df.select_dtypes(include=np.number).columns:\n",
        "            df[col] = df[col].fillna(df[col].mean())\n",
        "        for col in df.select_dtypes(include='object').columns:\n",
        "            df[col] = df[col].fillna(df[col].mode()[0])\n",
        "        if 'weight' in df.columns and 'height' in df.columns:\n",
        "            df['bmi'] = df['weight'] / (df['height'] / 100)**2\n",
        "        return df\n",
        "\n",
        "    def split_data(self, df: pd.DataFrame, target_column: Union[str, List[str]], test_size: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
        "        X = df.drop(columns=target_column, errors='ignore')\n",
        "        y = df[target_column] if isinstance(target_column, str) and target_column in df.columns else df[target_column] if isinstance(target_column, list) and all(col in df.columns for col in target_column) else None\n",
        "        if y is None:\n",
        "            raise ValueError(f\"Target column(s) '{target_column}' not found in DataFrame.\")\n",
        "        return train_test_split(X, y, test_size=test_size, random_state=config.RANDOM_STATE)\n",
        "\n",
        "    def create_preprocessing_pipeline(self, X: pd.DataFrame) -> ColumnTransformer:\n",
        "        numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "        categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "        numerical_transformer = StandardScaler()\n",
        "        categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_features),\n",
        "                ('cat', categorical_transformer, categorical_features)])\n",
        "        return preprocessor\n",
        "\n",
        "# --- 4. Model Training Module ---\n",
        "class ModelTrainer:\n",
        "    \"\"\"Trains and evaluates machine learning models.\"\"\"\n",
        "    def __init__(self, model_type: str):\n",
        "        self.model_type = model_type\n",
        "        self.model = self._initialize_model()\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        if self.model_type == 'fitness_level':\n",
        "            if config.FITNESS_LEVEL_MODEL_TYPE == 'random_forest':\n",
        "                return RandomForestClassifier(random_state=config.RANDOM_STATE)\n",
        "            elif config.FITNESS_LEVEL_MODEL_TYPE == 'linear_regression':\n",
        "                return LinearRegression() # Consider Logistic Regression for classification\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported model type for fitness level: {config.FITNESS_LEVEL_MODEL_TYPE}\")\n",
        "        elif self.model_type == 'training_params':\n",
        "            if config.TRAINING_PARAMS_MODEL_TYPE == 'random_forest':\n",
        "                return RandomForestRegressor(random_state=config.RANDOM_STATE)\n",
        "            elif config.TRAINING_PARAMS_MODEL_TYPE == 'linear_regression':\n",
        "                return LinearRegression()\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported model type for training parameters: {config.TRAINING_PARAMS_MODEL_TYPE}\")\n",
        "        elif self.model_type == 'dietary_needs':\n",
        "            if config.DIETARY_NEEDS_MODEL_TYPE == 'linear_regression':\n",
        "                return LinearRegression()\n",
        "            elif config.DIETARY_NEEDS_MODEL_TYPE == 'random_forest':\n",
        "                return RandomForestRegressor(random_state=config.RANDOM_STATE)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported model type for dietary needs: {config.DIETARY_NEEDS_MODEL_TYPE}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "    def train_model(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "        logging.info(f\"Training the {self.model_type} model...\")\n",
        "        self.model.fit(X_train, y_train)\n",
        "        logging.info(f\"{self.model_type} model training complete.\")\n",
        "\n",
        "    def evaluate_model(self, X_test: pd.DataFrame, y_test: pd.Series):\n",
        "        logging.info(f\"Evaluating the {self.model_type} model...\")\n",
        "        if self.model_type == 'fitness_level':\n",
        "            y_pred = self.model.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            report = classification_report(y_test, y_pred)\n",
        "            logging.info(f\"{self.model_type} model accuracy: {accuracy:.4f}\")\n",
        "            logging.info(f\"{self.model_type} model classification report:\\n{report}\")\n",
        "            return accuracy\n",
        "        elif self.model_type == 'training_params' or self.model_type == 'dietary_needs':\n",
        "            y_pred = self.model.predict(X_test)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            logging.info(f\"{self.model_type} model mean squared error: {mse:.4f}\")\n",
        "            logging.info(f\"{self.model_type} model R-squared: {r2:.4f}\")\n",
        "            return mse\n",
        "        return None\n",
        "\n",
        "    def save_model(self, filename: str):\n",
        "        model_path = os.path.join(config.MODEL_DIR, filename)\n",
        "        try:\n",
        "            joblib.dump(self.model, model_path)\n",
        "            logging.info(f\"{self.model_type} model saved to: {model_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving {self.model_type} model: {e}\")\n",
        "\n",
        "    def load_model(self, filename: str):\n",
        "        model_path = os.path.join(config.MODEL_DIR, filename)\n",
        "        try:\n",
        "            self.model = joblib.load(model_path)\n",
        "            logging.info(f\"{self.model_type} model loaded from: {model_path}\")\n",
        "        except FileNotFoundError:\n",
        "            logging.error(f\"Model file not found at: {model_path}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading {self.model_type} model: {e}\")\n",
        "\n",
        "# --- 5. Hyperparameter Tuning Module ---\n",
        "class HyperparameterTuner:\n",
        "    \"\"\"Tunes the hyperparameters of the AI models using GridSearchCV.\"\"\"\n",
        "    def __init__(self, model_type: str, model, param_grid: Dict):\n",
        "        self.model_type = model_type\n",
        "        self.model = model\n",
        "        self.param_grid = param_grid\n",
        "        self.best_model = None\n",
        "\n",
        "    def tune_hyperparameters(self, X_train: pd.DataFrame, y_train: pd.Series, scoring: str = None, cv: int = config.N_SPLITS_CV):\n",
        "        logging.info(f\"Tuning hyperparameters for the {self.model_type} model...\")\n",
        "        grid_search = GridSearchCV(estimator=self.model, param_grid=self.param_grid, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        self.best_model = grid_search.best_estimator_\n",
        "        logging.info(f\"Best hyperparameters for {self.model_type}: {grid_search.best_params_}\")\n",
        "\n",
        "    def get_best_model(self):\n",
        "        return self.best_model\n",
        "\n",
        "# --- 6. Prediction Module ---\n",
        "class PredictionEngine:\n",
        "    \"\"\"Handles loading trained models and making predictions.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.fitness_level_model = None\n",
        "        self.training_params_model = None\n",
        "        self.dietary_needs_model = None\n",
        "        self.preprocessor = DataPreprocessor()\n",
        "        self.feature_encoders = {}  # Store fitted preprocessors\n",
        "        self.exercise_database = self._load_exercise_database()\n",
        "\n",
        "    def _load_exercise_database(self) -> Dict:\n",
        "        \"\"\"Loads the exercise database from the configured CSV file.\"\"\"\n",
        "        exercise_db = {}\n",
        "        file_path = os.path.join(config.DATA_DIR, config.EXERCISE_DATABASE_FILE)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            for index, row in df.iterrows():\n",
        "                exercise_db[row['name']] = row.drop('name').to_dict()\n",
        "            logging.info(f\"Loaded exercise database from: {file_path}\")\n",
        "        except FileNotFoundError:\n",
        "            logging.warning(f\"Exercise database file not found at: {file_path}. Training plan will have basic info.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading exercise database: {e}\")\n",
        "        return exercise_db\n",
        "\n",
        "    def load_models(self):\n",
        "        try:\n",
        "            trainer_fitness = ModelTrainer(model_type='fitness_level')\n",
        "            trainer_fitness.load_model(config.FITNESS_LEVEL_MODEL_NAME)\n",
        "            self.fitness_level_model = trainer_fitness.model\n",
        "\n",
        "            trainer_training_params = ModelTrainer(model_type='training_params')\n",
        "            trainer_training_params.load_model(config.TRAINING_PARAMS_MODEL_NAME)\n",
        "            self.training_params_model = trainer_training_params.model\n",
        "\n",
        "            trainer_dietary_needs = ModelTrainer(model_type='dietary_needs')\n",
        "            trainer_dietary_needs.load_model(config.DIETARY_NEEDS_MODEL_NAME)\n",
        "            self.dietary_needs_model = trainer_dietary_needs.model\n",
        "\n",
        "            # Load the fitted preprocessors\n",
        "            self.feature_encoders['fitness_level'] = joblib.load(os.path.join(config.MODEL_DIR, 'fitness_preprocessor.pkl'))\n",
        "            self.feature_encoders['training_params'] = joblib.load(os.path.join(config.MODEL_DIR, 'training_preprocessor.pkl'))\n",
        "            self.feature_encoders['dietary_needs'] = joblib.load(os.path.join(config.MODEL_DIR, 'dietary_preprocessor.pkl'))\n",
        "\n",
        "            logging.info(\"All models loaded successfully.\")\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            logging.error(f\"Error loading models: {e}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An unexpected error occurred while loading models: {e}\")\n",
        "            raise\n",
        "\n",
        "    def predict_fitness_level(self, user_profile: Dict) -> str:\n",
        "        if self.fitness_level_model is None or 'fitness_level' not in self.feature_encoders:\n",
        "            logging.error(\"Fitness level model or preprocessor not loaded.\")\n",
        "            return \"Error\"\n",
        "        try:\n",
        "            user_df = pd.DataFrame([user_profile])\n",
        "            feature_names = joblib.load(os.path.join(config.MODEL_DIR, 'fitness_feature_names.pkl'))\n",
        "            common_features = [feature for feature in feature_names if feature in user_df.columns]\n",
        "            if not common_features:\n",
        "                logging.warning(f\"Missing features in user profile for fitness level prediction: {set(feature_names) - set(user_df.columns)}\")\n",
        "                user_df = user_df.reindex(columns=feature_names, fill_value=0) # Fill missing with 0 (handle with care)\n",
        "            else:\n",
        "                user_df = user_df[common_features] # Use only available features\n",
        "\n",
        "            processed_data = self.feature_encoders['fitness_level'].transform(user_df)\n",
        "            prediction = self.fitness_level_model.predict(processed_data)[0]\n",
        "            return prediction\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error predicting fitness level: {e}\")\n",
        "            return \"Error\"\n",
        "\n",
        "    def predict_training_plan(self, user_profile: Dict) -> Dict:\n",
        "        if self.training_params_model is None or 'training_params' not in self.feature_encoders:\n",
        "            logging.error(\"Training parameters model or preprocessor not loaded.\")\n",
        "            return {\"error\": \"Model not loaded\"}\n",
        "        try:\n",
        "            user_df = pd.DataFrame([user_profile])\n",
        "            feature_names = joblib.load(os.path.join(config.MODEL_DIR, 'training_params_feature_names.pkl'))\n",
        "            common_features = [feature for feature in feature_names if feature in user_df.columns]\n",
        "            if not common_features:\n",
        "                logging.warning(f\"Missing features in user profile for training plan prediction: {set(feature_names) - set(user_df.columns)}\")\n",
        "                user_df = user_df.reindex(columns=feature_names, fill_value=0) # Fill missing with 0 (handle with care)\n",
        "            else:\n",
        "                user_df = user_df[common_features] # Use only available features\n",
        "\n",
        "            processed_data = self.feature_encoders['training_params'].transform(user_df)\n",
        "            prediction = self.training_params_model.predict(processed_data)[0].tolist()\n",
        "\n",
        "            plan = {\"workout_frequency\": config.TRAINING_PLAN_CONFIG.get(\"workout_frequency\", \"3 days per week\"), \"exercises\": []}\n",
        "\n",
        "            for exercise_config in config.TRAINING_PLAN_CONFIG.get(\"exercises\", []):\n",
        "                name = exercise_config.get(\"name\")\n",
        "                sets_index = exercise_config.get(\"sets_index\")\n",
        "                reps_index = exercise_config.get(\"reps_index\")\n",
        "                if name is not None and sets_index is not None and reps_index is not None and sets_index < len(prediction) and reps_index < len(prediction):\n",
        "                    exercise_info = self.exercise_database.get(name, {})\n",
        "                    plan[\"exercises\"].append({\n",
        "                        \"name\": name,\n",
        "                        \"sets\": round(prediction[sets_index]),\n",
        "                        \"repetitions\": round(prediction[reps_index]),\n",
        "                        \"details\": exercise_info.get(\"description\"),\n",
        "                        \"muscle_group\": exercise_info.get(\"muscle_group\")\n",
        "                        # Add more details from exercise_info as needed\n",
        "                    })\n",
        "                elif name:\n",
        "                    logging.warning(f\"Could not generate parameters for {name}.\")\n",
        "\n",
        "            return plan\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error predicting training plan: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def predict_dietary_needs(self, user_profile: Dict) -> Dict:\n",
        "        if self.dietary_needs_model is None or 'dietary_needs' not in self.feature_encoders:\n",
        "            logging.error(\"Dietary needs model or preprocessor not loaded.\")\n",
        "            return {\"error\": \"Model not loaded\"}\n",
        "        try:\n",
        "            user_df = pd.DataFrame([user_profile])\n",
        "            feature_names = joblib.load(os.path.join(config.MODEL_DIR, 'dietary_needs_feature_names.pkl'))\n",
        "            common_features = [feature for feature in feature_names if feature in user_df.columns]\n",
        "            if not common_features:\n",
        "                logging.warning(f\"Missing features in user profile for dietary needs prediction: {set(feature_names) - set(user_df.columns)}\")\n",
        "                user_df = user_df.reindex(columns=feature_names, fill_value=0) # Fill missing with 0 (handle with care)\n",
        "            else:\n",
        "                user_df = user_df[common_features] # Use only available features\n",
        "            processed_data = self.feature_encoders['dietary_needs'].transform(user_df)\n",
        "            prediction = self.dietary_needs_model.predict(processed_data)[0].tolist() # Assuming output is [calories, protein, carbs, fat]\n",
        "            needs = {\n",
        "                \"daily_calories\": round(prediction[0]),\n",
        "                \"macronutrient_targets\": {\n",
        "                    \"protein\": round(prediction[1]),\n",
        "                    \"carbs\": round(prediction[2]),\n",
        "                    \"fat\": round(prediction[3])\n",
        "                }\n",
        "            }\n",
        "            return needs\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error predicting dietary needs: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "# --- 7. Training Function ---\n",
        "def train_models(config: Configuration):\n",
        "    \"\"\"Trains and saves the AI models.\"\"\"\n",
        "    preprocessor = DataPreprocessor()\n",
        "\n",
        "    # --- Hyperparameter Tuning Example (Uncomment and configure to use) ---\n",
        "    # from sklearn.model_selection import ParameterGrid\n",
        "    #\n",
        "    # # Define parameter grid for fitness level model\n",
        "    # fitness_param_grid = {'n_estimators': [50, 100], 'max_depth': [5, 10, None]}\n",
        "    # tuner_fitness = HyperparameterTuner(model_type='fitness_level', model=RandomForestClassifier(random_state=config.RANDOM_STATE), param_grid=fitness_param_grid)\n",
        "    # # Assuming you have X_train_processed_fitness and y_train_fitness from the data loading and preprocessing step\n",
        "    # tuner_fitness.tune_hyperparameters(X_train_processed_fitness, y_train_fitness, scoring='accuracy')\n",
        "    # best_fitness_model = tuner_fitness.get_best_model()\n",
        "    # if best_fitness_model:\n",
        "    #     trainer_fitness.model = best_fitness_model\n",
        "    #     logging.info(f\"Using best hyperparameters for fitness level: {tuner_fitness.best_model.get_params()}\")\n",
        "\n",
        "    # --- Train Fitness Level Classification Model ---\n",
        "    try:\n",
        "        # expected_cols_fitness = ['age', 'gender', 'weight', 'height', 'pushups', 'squats', 'weight_lifted_squat_max', 'weight_lifted_bench_max', 'activity_level', 'fitness_level'] # Adjust based on your data\n",
        "        fitness_data = preprocessor.load_data(os.path.join(config.DATA_DIR, config.FITNESS_LEVEL_DATA_FILE)) # Removed expected_cols for example data\n",
        "        fitness_data = preprocessor.preprocess_user_data(fitness_data)\n",
        "        X_fitness, y_fitness = fitness_data.drop('fitness_level', axis=1, errors='ignore'), fitness_data['fitness_level']\n",
        "        X_train_fitness, X_test_fitness, y_train_fitness, y_test_fitness = preprocessor.split_data(fitness_data, 'fitness_level')\n",
        "\n",
        "        fitness_preprocessor = preprocessor.create_preprocessing_pipeline(X_train_fitness)\n",
        "        X_train_processed_fitness = fitness_preprocessor.fit_transform(X_train_fitness)\n",
        "        X_test_processed_fitness = fitness_preprocessor.transform(X_test_fitness)\n",
        "\n",
        "        trainer_fitness = ModelTrainer(model_type='fitness_level')\n",
        "        trainer_fitness.train_model(X_train_processed_fitness, y_train_fitness)\n",
        "        trainer_fitness.evaluate_model(X_test_processed_fitness, y_test_fitness)\n",
        "        trainer_fitness.save_model(config.FITNESS_LEVEL_MODEL_NAME)\n",
        "        joblib.dump(fitness_preprocessor, os.path.join(config.MODEL_DIR, 'fitness_preprocessor.pkl'))\n",
        "        joblib.dump(X_train_fitness.columns.tolist(), os.path.join(config.MODEL_DIR, 'fitness_feature_names.pkl'))\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.warning(\"Fitness level training data not found. Skipping training.\")\n",
        "    except ValueError as ve:\n",
        "        logging.error(f\"Data validation error for fitness level model: {ve}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training fitness level model: {e}\")\n",
        "\n",
        "    # --- Train Training Parameters Regression Model ---\n",
        "    try:\n",
        "        target_columns_tp = ['squat_sets', 'squat_reps', 'bench_sets', 'bench_reps', 'deadlift_sets', 'deadlift_reps', 'overhead_sets', 'overhead_reps'] # Adjust based on your data\n",
        "        # expected_cols_tp = ['age', 'gender', 'weight', 'height', 'activity_level'] + target_columns_tp # Add all expected columns\n",
        "        training_params_data = preprocessor.load_data(os.path.join(config.DATA_DIR, config.TRAINING_PARAMS_DATA_FILE)) # Removed expected_cols for example data\n",
        "        training_params_data = preprocessor.preprocess_user_data(training_params_data)\n",
        "        X_training_params, y_training_params = training_params_data.drop(target_columns_tp, axis=1, errors='ignore'), training_params_data[target_columns_tp]\n",
        "        X_train_tp, X_test_tp, y_train_tp, y_test_tp = preprocessor.split_data(training_params_data, target_columns_tp)\n",
        "\n",
        "        training_preprocessor = preprocessor.create_preprocessing_pipeline(X_train_tp)\n",
        "        X_train_processed_tp = training_preprocessor.fit_transform(X_train_tp)\n",
        "        X_test_processed_tp = training_preprocessor.transform(X_test_tp)\n",
        "\n",
        "        trainer_training_params = ModelTrainer(model_type='training_params')\n",
        "        trainer_training_params.train_model(X_train_processed_tp, y_train_tp)\n",
        "        trainer_training_params.evaluate_model(X_test_processed_tp, y_test_tp)\n",
        "        trainer_training_params.save_model(config.TRAINING_PARAMS_MODEL_NAME)\n",
        "        joblib.dump(training_preprocessor, os.path.join(config.MODEL_DIR, 'training_preprocessor.pkl'))\n",
        "        joblib.dump(X_train_tp.columns.tolist(), os.path.join(config.MODEL_DIR, 'training_params_feature_names.pkl'))\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.warning(\"Training parameters data not found. Skipping training.\")\n",
        "    except ValueError as ve:\n",
        "        logging.error(f\"Data validation error for training parameters model: {ve}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training training parameters model: {e}\")\n",
        "\n",
        "    # --- Train Dietary Needs Regression Model ---\n",
        "    try:\n",
        "        target_columns_dn = ['calories', 'protein', 'carbs', 'fat']\n",
        "        # expected_cols_dn = ['age', 'gender', 'weight', 'height', 'activity_level'] + target_columns_dn\n",
        "        dietary_needs_data = preprocessor.load_data(os.path.join(config.DATA_DIR, config.DIETARY_NEEDS_DATA_FILE)) # Removed expected_cols for example data\n",
        "        dietary_needs_data = preprocessor.preprocess_user_data(dietary_needs_data)\n",
        "        X_dietary_needs, y_dietary_needs = dietary_needs_data.drop(target_columns_dn, axis=1, errors='ignore'), dietary_needs_data[target_columns_dn]\n",
        "        X_train_dn, X_test_dn, y_train_dn, y_test_dn = preprocessor.split_data(dietary_needs_data, target_columns_dn)\n",
        "\n",
        "        dietary_preprocessor = preprocessor.create_preprocessing_pipeline(X_train_dn)\n",
        "        X_train_processed_dn = dietary_preprocessor.fit_transform(X_train_dn)\n",
        "        X_test_processed_dn = dietary_preprocessor.transform(X_test_dn)\n",
        "\n",
        "        trainer_dietary_needs = ModelTrainer(model_type='dietary_needs')\n",
        "        trainer_dietary_needs.train_model(X_train_processed_dn, y_train_dn)\n",
        "        trainer_dietary_needs.evaluate_model(X_test_processed_dn, y_test_dn)\n",
        "        trainer_dietary_needs.save_model(config.DIETARY_NEEDS_MODEL_NAME)\n",
        "        joblib.dump(dietary_preprocessor, os.path.join(config.MODEL_DIR, 'dietary_preprocessor.pkl'))\n",
        "        joblib.dump(X_train_dn.columns.tolist(), os.path.join(config.MODEL_DIR, 'dietary_needs_feature_names.pkl'))\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.warning(\"Dietary needs training data not found. Skipping training.\")\n",
        "    except ValueError as ve:\n",
        "        logging.error(f\"Data validation error for dietary needs model: {ve}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training dietary needs model: {e}\")\n",
        "\n",
        "# --- 8. TensorFlow Lite Conversion Function ---\n",
        "def convert_sklearn_model_to_tflite(model_path: str, output_path: str, input_shape: Tuple):\n",
        "    \"\"\"Converts a trained scikit-learn model to TensorFlow Lite format.\"\"\"\n",
        "    try:\n",
        "        # Load the scikit-learn model\n",
        "        sklearn_model = joblib.load(model_path)\n",
        "\n",
        "        # Define the input specification for ONNX conversion\n",
        "        initial_type = [('float_input', FloatTensorType(input_shape))]\n",
        "\n",
        "        # Convert the scikit-learn model to ONNX format\n",
        "        onnx_model = convert_sklearn(sklearn_model, initial_types=initial_type)\n",
        "        onnx_file_path = output_path.replace(\".tflite\", \".onnx\")\n",
        "        with open(onnx_file_path, \"wb\") as f:\n",
        "            f.write(onnx_model.SerializeToString())\n",
        "        logging.info(f\"ONNX model saved to: {onnx_file_path}\")\n",
        "\n",
        "        # Convert the ONNX model to TensorFlow Lite\n",
        "        converter = tf.lite.TFLiteConverter.from_onnx(onnx_file_path)\n",
        "        tflite_model = converter.convert()\n",
        "\n",
        "        # Save the TensorFlow Lite model\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "        logging.info(f\"TensorFlow Lite model saved to: {output_path}\")\n",
        "        os.remove(onnx_file_path) # Clean up the ONNX file\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Model not found at: {model_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting {model_path} to TensorFlow Lite: {e}\")\n",
        "\n",
        "def convert_sklearn_to_tflite(config: Configuration):\n",
        "    \"\"\"Converts trained scikit-learn models to TensorFlow Lite format.\"\"\"\n",
        "    # Ensure TensorFlow version is compatible with ONNX conversion (>= 2.7 recommended)\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        if tf.__version__ < '2.7':\n",
        "            logging.warning(f\"TensorFlow version {tf.__version__} might be older than recommended for ONNX to TFLite conversion. Consider upgrading to 2.7 or higher.\")\n",
        "    except ImportError:\n",
        "        logging.error(\"TensorFlow not found.\")\n",
        "        return\n",
        "\n",
        "    convert_sklearn_model_to_tflite(\n",
        "        os.path.join(config.MODEL_DIR, config.FITNESS_LEVEL_MODEL_NAME),\n",
        "        os.path.join(config.TFLITE_MODEL_DIR, 'fitness_level_model.tflite'),\n",
        "        (None, joblib.load(os.path.join(config.MODEL_DIR, 'fitness_feature_names.pkl')).__len__())\n",
        "    )\n",
        "    convert_sklearn_model_to_tflite(\n",
        "        os.path.join(config.MODEL_DIR, config.TRAINING_PARAMS_MODEL_NAME),\n",
        "        os.path.join(config.TFLITE_MODEL_DIR, 'training_params_model.tflite'),\n",
        "        (None, joblib.load(os.path.join(config.MODEL_DIR, 'training_params_feature_names.pkl')).__len__())\n",
        "    )\n",
        "    convert_sklearn_model_to_tflite(\n",
        "        os.path.join(config.MODEL_DIR, config.DIETARY_NEEDS_MODEL_NAME),\n",
        "        os.path.join(config.TFLITE_MODEL_DIR, 'dietary_needs_model.tflite'),\n",
        "        (None, joblib.load(os.path.join(config.MODEL_DIR, 'dietary_needs_feature_names.pkl')).__len__())\n",
        "    )\n",
        "\n",
        "# --- 9. Main Function to Run Predictions ---\n",
        "def main():\n",
        "    \"\"\"Loads trained models and demonstrates prediction for a sample user.\"\"\"\n",
        "    prediction_engine = PredictionEngine()\n",
        "    preprocessor = DataPreprocessor() # Instantiate DataPreprocessor here\n",
        "    try:\n",
        "        prediction_engine.load_models()\n",
        "\n",
        "        # Example user profile (replace with actual user input)\n",
        "        sample_user_profile = {\n",
        "            \"age\": 30,\n",
        "            \"gender\": \"male\",\n",
        "            \"weight\": 80,   # kg\n",
        "            \"height\": 180,  # cm\n",
        "            \"pushups\": 10,\n",
        "            \"squats\": 20,\n",
        "            \"weight_lifted_squat_max\": 90.0,\n",
        "            \"weight_lifted_bench_max\": 70.0,\n",
        "            \"activity_level\": \"Moderately Active\",\n",
        "            \"target_goal\": \"strength\", # Example for dietary needs and training parameters\n",
        "            \"dietary_preference\": \"none\", # Example for dietary needs\n",
        "            # Add other features based on your training data. Ensure these match the features used during training.\n",
        "            \"plank_duration\": 60,\n",
        "            \"running_endurance\": 15,\n",
        "            \"weight_lifted_squat\": 100,\n",
        "        }\n",
        "\n",
        "        # Preprocess the sample user profile to calculate 'bmi'\n",
        "        sample_user_profile_df = pd.DataFrame([sample_user_profile])\n",
        "        processed_user_profile_df = preprocessor.preprocess_user_data(sample_user_profile_df)\n",
        "        processed_user_profile = processed_user_profile_df.iloc[0].to_dict()\n",
        "\n",
        "        fitness_level = prediction_engine.predict_fitness_level(processed_user_profile)\n",
        "        print(f\"\\nPredicted Fitness Level: {fitness_level}\")\n",
        "\n",
        "        training_plan = prediction_engine.predict_training_plan(processed_user_profile)\n",
        "        print(\"\\n--- Predicted Training Plan ---\")\n",
        "        print(training_plan)\n",
        "\n",
        "        dietary_needs = prediction_engine.predict_dietary_needs(processed_user_profile)\n",
        "        print(\"\\n--- Predicted Dietary Needs ---\")\n",
        "        print(dietary_needs)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during prediction: {e}\")\n",
        "\n",
        "# --- 10. Conceptual Deployment (Illustrative) ---\n",
        "def deploy_model_conceptual(config: Configuration):\n",
        "    \"\"\"Conceptual model deployment function.\"\"\"\n",
        "    logging.info(\"Conceptual model deployment started...\")\n",
        "    logging.info(f\"Trained scikit-learn models are saved in: {config.MODEL_DIR}\")\n",
        "    logging.info(f\"TensorFlow Lite models are saved in: {config.TFLITE_MODEL_DIR}\")\n",
        "    logging.info(\"Conceptual model deployment finished.\")\n",
        "\n",
        "# --- 11. Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Ensure TensorFlow version is set (run this if you haven't already)\n",
        "    # %pip install tensorflow>=2.7\n",
        "    # import tensorflow as tf\n",
        "    # print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "    # To train the models and convert them to TensorFlow Lite, uncomment the following lines:\n",
        "    train_models(config)\n",
        "    convert_sklearn_to_tflite(config)\n",
        "\n",
        "    # To run predictions using the trained scikit-learn models:\n",
        "    main()\n",
        "\n",
        "    # To simulate deployment (conceptual):\n",
        "    deploy_model_conceptual(config)"
      ]
    }
  ]
}