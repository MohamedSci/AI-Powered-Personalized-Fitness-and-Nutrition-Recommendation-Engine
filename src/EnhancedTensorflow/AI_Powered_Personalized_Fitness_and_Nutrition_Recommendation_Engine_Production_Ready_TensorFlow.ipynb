{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Installation ---\n",
        "!pip install -U tensorflow\n",
        "!pip install -U torch\n",
        "!pip install -U skl2onnx\n",
        "!pip install -U tf2onnx"
      ],
      "metadata": {
        "id": "M39aEhyrbiPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report, r2_score\n",
        "import joblib\n",
        "import logging\n",
        "import os\n",
        "from typing import Dict, List, Union, Tuple, Any\n",
        "from dataclasses import dataclass, field\n",
        "import tensorflow as tf\n",
        "from skl2onnx import convert_sklearn\n",
        "from skl2onnx.common.data_types import FloatTensorType"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twy4zbIVblxQ",
        "outputId": "1f84d48a-347c-44b3-c76b-9a999aa0d3dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Configuration Management ---\n",
        "@dataclass\n",
        "class Configuration:\n",
        "    \"\"\"Configuration class to manage file paths, hyperparameters, and model settings.\"\"\"\n",
        "    DRIVE_ROOT: str = '/content/drive/MyDrive'\n",
        "    PROJECT_FOLDER: str = 'Projects/Get_Fit_App/Ai_Model'\n",
        "    DATA_FOLDER: str = 'Data_Source'\n",
        "    DATA_DIR: str = os.path.join(DRIVE_ROOT, PROJECT_FOLDER, DATA_FOLDER)\n",
        "    MODEL_DIR: str = os.path.join(DRIVE_ROOT, PROJECT_FOLDER, 'models')  # Folder for original scikit-learn models\n",
        "    TFLITE_MODEL_DIR: str = os.path.join(DRIVE_ROOT, PROJECT_FOLDER, 'tflite_models')  # Folder for TensorFlow Lite models\n",
        "    FITNESS_LEVEL_DATA_FILE: str = 'fitness_level_data_example.csv'\n",
        "    TRAINING_PARAMS_DATA_FILE: str = 'training_params_data_example.csv'\n",
        "    DIETARY_NEEDS_DATA_FILE: str = 'dietary_needs_data_example.csv'\n",
        "    EXERCISE_DATABASE_FILE: str = 'exercise_database_example.csv'\n",
        "    FITNESS_LEVEL_MODEL_NAME: str = 'fitness_level_model.pkl'\n",
        "    TRAINING_PARAMS_MODEL_NAME: str = 'training_params_model.pkl'\n",
        "    DIETARY_NEEDS_MODEL_NAME: str = 'dietary_needs_model.pkl'\n",
        "    N_SPLITS_CV: int = 5\n",
        "    RANDOM_STATE: int = 42\n",
        "    LOG_LEVEL: int = logging.INFO\n",
        "    FITNESS_LEVEL_MODEL_TYPE: str = 'random_forest'  # Options: 'random_forest', 'logistic_regression'\n",
        "    TRAINING_PARAMS_MODEL_TYPE: str = 'random_forest' # Options: 'random_forest', 'linear_regression'\n",
        "    DIETARY_NEEDS_MODEL_TYPE: str = 'linear_regression' # Options: 'linear_regression', 'random_forest'\n",
        "    TRAINING_PLAN_CONFIG: Dict = field(default_factory=lambda: {\n",
        "        \"workout_frequency\": \"3 days per week\",\n",
        "        \"exercises\": [\n",
        "            {\"name\": \"Barbell Squat\", \"sets_index\": 0, \"reps_index\": 1},\n",
        "            {\"name\": \"Bench Press\", \"sets_index\": 2, \"reps_index\": 3},\n",
        "            {\"name\": \"Deadlift\", \"sets_index\": 4, \"reps_index\": 5},\n",
        "            {\"name\": \"Overhead Press\", \"sets_index\": 6, \"reps_index\": 7}\n",
        "            # Add more exercises and their corresponding prediction indices based on your model output\n",
        "        ]\n",
        "    })\n",
        "    IMPUTATION_STRATEGY_NUMERICAL: str = 'mean' # Options: 'mean', 'median', 'constant'\n",
        "    IMPUTATION_STRATEGY_CATEGORICAL: str = 'most_frequent' # Options: 'most_frequent', 'constant'\n",
        "\n",
        "# Initialize configuration and logging\n",
        "config = Configuration()\n",
        "os.makedirs(config.MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(config.TFLITE_MODEL_DIR, exist_ok=True)\n",
        "\n",
        "logging.basicConfig(level=config.LOG_LEVEL, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')"
      ],
      "metadata": {
        "id": "urJI9frHbo-f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Data Loading and Preprocessing Module ---\n",
        "class DataPreprocessor:\n",
        "    \"\"\"Handles loading, validation, and preprocessing of data.\"\"\"\n",
        "    def load_data(self, file_path: str, expected_columns: List[str] = None) -> pd.DataFrame:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            if df.empty:\n",
        "                logging.warning(f\"Loaded data from {file_path} is empty.\")\n",
        "            if expected_columns:\n",
        "                missing_columns = [col for col in expected_columns if col not in df.columns]\n",
        "                if missing_columns:\n",
        "                    logging.error(f\"Missing expected columns in {file_path}: {missing_columns}\")\n",
        "                    raise ValueError(f\"Missing columns: {missing_columns}\")\n",
        "                logging.info(f\"Data loaded from {file_path} with expected columns.\")\n",
        "            else:\n",
        "                logging.info(f\"Data loaded from {file_path}.\")\n",
        "            return df\n",
        "        except FileNotFoundError:\n",
        "            logging.error(f\"Data file not found at: {file_path}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading data from {file_path}: {e}\")\n",
        "            raise\n",
        "\n",
        "    def preprocess_user_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        logging.info(\"Preprocessing user data...\")\n",
        "        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "        for col in df.select_dtypes(include=np.number).columns:\n",
        "            if config.IMPUTATION_STRATEGY_NUMERICAL == 'mean':\n",
        "                df[col] = df[col].fillna(df[col].mean())\n",
        "            elif config.IMPUTATION_STRATEGY_NUMERICAL == 'median':\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "            elif config.IMPUTATION_STRATEGY_NUMERICAL == 'constant':\n",
        "                df[col] = df[col].fillna(0) # You might want to configure this constant\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported numerical imputation strategy: {config.IMPUTATION_STRATEGY_NUMERICAL}\")\n",
        "        for col in df.select_dtypes(include='object').columns:\n",
        "            if config.IMPUTATION_STRATEGY_CATEGORICAL == 'most_frequent':\n",
        "                df[col] = df[col].fillna(df[col].mode()[0])\n",
        "            elif config.IMPUTATION_STRATEGY_CATEGORICAL == 'constant':\n",
        "                df[col] = df[col].fillna('unknown') # You might want to configure this constant\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported categorical imputation strategy: {config.IMPUTATION_STRATEGY_CATEGORICAL}\")\n",
        "        if 'weight' in df.columns and 'height' in df.columns:\n",
        "            # Assuming height is in cm, converting to meters\n",
        "            df['bmi'] = df['weight'] / (df['height'] / 100)**2\n",
        "        return df\n",
        "\n",
        "    def split_data(self, df: pd.DataFrame, target_column: Union[str, List[str]], test_size: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
        "        X = df.drop(columns=target_column, errors='ignore')\n",
        "        y = df[target_column] if isinstance(target_column, str) and target_column in df.columns else df[target_column] if isinstance(target_column, list) and all(col in df.columns for col in target_column) else None\n",
        "        if y is None:\n",
        "            raise ValueError(f\"Target column(s) '{target_column}' not found in DataFrame.\")\n",
        "        return train_test_split(X, y, test_size=test_size, random_state=config.RANDOM_STATE)\n",
        "\n",
        "    def create_preprocessing_pipeline(self, X: pd.DataFrame) -> ColumnTransformer:\n",
        "        numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "        categorical_features = X.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "        numerical_transformer = StandardScaler()\n",
        "        categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "        preprocessor = ColumnTransformer(\n",
        "            transformers=[\n",
        "                ('num', numerical_transformer, numerical_features),\n",
        "                ('cat', categorical_transformer, categorical_features)])\n",
        "        return preprocessor"
      ],
      "metadata": {
        "id": "Vq4W2fBFbs0Z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Model Training Module ---\n",
        "class ModelTrainer:\n",
        "    \"\"\"Trains and evaluates machine learning models.\"\"\"\n",
        "    def __init__(self, model_type: str):\n",
        "        self.model_type = model_type\n",
        "        self.model = self._initialize_model()\n",
        "\n",
        "    def _initialize_model(self):\n",
        "        if self.model_type == 'fitness_level':\n",
        "            if config.FITNESS_LEVEL_MODEL_TYPE == 'random_forest':\n",
        "                return RandomForestClassifier(random_state=config.RANDOM_STATE)\n",
        "            elif config.FITNESS_LEVEL_MODEL_TYPE == 'logistic_regression':\n",
        "                return LogisticRegression(random_state=config.RANDOM_STATE, solver='liblinear') # Recommended solver for small datasets\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported model type for fitness level: {config.FITNESS_LEVEL_MODEL_TYPE}\")\n",
        "        elif self.model_type == 'training_params':\n",
        "            if config.TRAINING_PARAMS_MODEL_TYPE == 'random_forest':\n",
        "                return RandomForestRegressor(random_state=config.RANDOM_STATE)\n",
        "            elif config.TRAINING_PARAMS_MODEL_TYPE == 'linear_regression':\n",
        "                return LinearRegression()\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported model type for training parameters: {config.TRAINING_PARAMS_MODEL_TYPE}\")\n",
        "        elif self.model_type == 'dietary_needs':\n",
        "            if config.DIETARY_NEEDS_MODEL_TYPE == 'linear_regression':\n",
        "                return LinearRegression()\n",
        "            elif config.DIETARY_NEEDS_MODEL_TYPE == 'random_forest':\n",
        "                return RandomForestRegressor(random_state=config.RANDOM_STATE)\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported model type for dietary needs: {config.DIETARY_NEEDS_MODEL_TYPE}\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model type: {self.model_type}\")\n",
        "\n",
        "    def train_model(self, X_train: pd.DataFrame, y_train: pd.Series):\n",
        "        logging.info(f\"Training the {self.model_type} model...\")\n",
        "        self.model.fit(X_train, y_train)\n",
        "        logging.info(f\"{self.model_type} model training complete.\")\n",
        "\n",
        "    def evaluate_model(self, X_test: pd.DataFrame, y_test: pd.Series):\n",
        "        logging.info(f\"Evaluating the {self.model_type} model...\")\n",
        "        if self.model_type == 'fitness_level':\n",
        "            y_pred = self.model.predict(X_test)\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "            report = classification_report(y_test, y_pred)\n",
        "            logging.info(f\"{self.model_type} model accuracy: {accuracy:.4f}\")\n",
        "            logging.info(f\"{self.model_type} model classification report:\\n{report}\")\n",
        "            return accuracy\n",
        "        elif self.model_type == 'training_params' or self.model_type == 'dietary_needs':\n",
        "            y_pred = self.model.predict(X_test)\n",
        "            mse = mean_squared_error(y_test, y_pred)\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "            logging.info(f\"{self.model_type} model mean squared error: {mse:.4f}\")\n",
        "            logging.info(f\"{self.model_type} model R-squared: {r2:.4f}\")\n",
        "            return mse\n",
        "        return None\n",
        "\n",
        "    def save_model(self, filename: str):\n",
        "        model_path = os.path.join(config.MODEL_DIR, filename)\n",
        "        try:\n",
        "            joblib.dump(self.model, model_path)\n",
        "            logging.info(f\"{self.model_type} model saved to: {model_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving {self.model_type} model: {e}\")\n",
        "\n",
        "    def load_model(self, filename: str):\n",
        "        model_path = os.path.join(config.MODEL_DIR, filename)\n",
        "        try:\n",
        "            self.model = joblib.load(model_path)\n",
        "            logging.info(f\"{self.model_type} model loaded from: {model_path}\")\n",
        "        except FileNotFoundError:\n",
        "            logging.error(f\"Model file not found at: {model_path}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading {self.model_type} model: {e}\")"
      ],
      "metadata": {
        "id": "Rz4B-A1GbxdI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Hyperparameter Tuning Module ---\n",
        "class HyperparameterTuner:\n",
        "    \"\"\"Tunes the hyperparameters of the AI models using GridSearchCV.\"\"\"\n",
        "    def __init__(self, model_type: str, model, param_grid: Dict):\n",
        "        self.model_type = model_type\n",
        "        self.model = model\n",
        "        self.param_grid = param_grid\n",
        "        self.best_model = None\n",
        "\n",
        "    def tune_hyperparameters(self, X_train: pd.DataFrame, y_train: pd.Series, scoring: str = None, cv: int = config.N_SPLITS_CV):\n",
        "        logging.info(f\"Tuning hyperparameters for the {self.model_type} model...\")\n",
        "        grid_search = GridSearchCV(estimator=self.model, param_grid=self.param_grid, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "        grid_search.fit(X_train, y_train)\n",
        "        self.best_model = grid_search.best_estimator_\n",
        "        logging.info(f\"Best hyperparameters for {self.model_type}: {grid_search.best_params_}\")\n",
        "\n",
        "    def get_best_model(self):\n",
        "        return self.best_model"
      ],
      "metadata": {
        "id": "bFdyGgQsb0mQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Prediction Module ---\n",
        "class PredictionEngine:\n",
        "    \"\"\"Handles loading trained models and making predictions.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.fitness_level_model = None\n",
        "        self.training_params_model = None\n",
        "        self.dietary_needs_model = None\n",
        "        self.preprocessor = DataPreprocessor()\n",
        "        self.feature_encoders = {}  # Store fitted preprocessors\n",
        "        self.exercise_database = self._load_exercise_database()\n",
        "\n",
        "    def _load_exercise_database(self) -> Dict:\n",
        "        \"\"\"Loads the exercise database from the configured CSV file.\"\"\"\n",
        "        exercise_db = {}\n",
        "        file_path = os.path.join(config.DATA_DIR, config.EXERCISE_DATABASE_FILE)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "            for index, row in df.iterrows():\n",
        "                exercise_db[row['exercise_name']] = row.drop('exercise_name').to_dict()\n",
        "            logging.info(f\"Loaded exercise database from: {file_path}\")\n",
        "        except FileNotFoundError:\n",
        "            logging.warning(f\"Exercise database file not found at: {file_path}. Training plan will have basic info.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error loading exercise database: {e}\")\n",
        "        return exercise_db\n",
        "\n",
        "    def load_models(self):\n",
        "        try:\n",
        "            trainer_fitness = ModelTrainer(model_type='fitness_level')\n",
        "            trainer_fitness.load_model(config.FITNESS_LEVEL_MODEL_NAME)\n",
        "            self.fitness_level_model = trainer_fitness.model\n",
        "\n",
        "            trainer_training_params = ModelTrainer(model_type='training_params')\n",
        "            trainer_training_params.load_model(config.TRAINING_PARAMS_MODEL_NAME)\n",
        "            self.training_params_model = trainer_training_params.model\n",
        "\n",
        "            trainer_dietary_needs = ModelTrainer(model_type='dietary_needs')\n",
        "            trainer_dietary_needs.load_model(config.DIETARY_NEEDS_MODEL_NAME)\n",
        "            self.dietary_needs_model = trainer_dietary_needs.model\n",
        "\n",
        "            # Load the fitted preprocessors\n",
        "            self.feature_encoders['fitness_level'] = joblib.load(os.path.join(config.MODEL_DIR, 'fitness_preprocessor.pkl'))\n",
        "            self.feature_encoders['training_params'] = joblib.load(os.path.join(config.MODEL_DIR, 'training_preprocessor.pkl'))\n",
        "            self.feature_encoders['dietary_needs'] = joblib.load(os.path.join(config.MODEL_DIR, 'dietary_preprocessor.pkl'))\n",
        "\n",
        "            logging.info(\"All models loaded successfully.\")\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            logging.error(f\"Error loading models: {e}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An unexpected error occurred while loading models: {e}\")\n",
        "            raise\n",
        "\n",
        "    def predict_fitness_level(self, user_profile: Dict) -> str:\n",
        "        if self.fitness_level_model is None or 'fitness_level' not in self.feature_encoders:\n",
        "            logging.error(\"Fitness level model or preprocessor not loaded.\")\n",
        "            return \"Error\"\n",
        "        try:\n",
        "            user_df = pd.DataFrame([user_profile])\n",
        "            feature_names = joblib.load(os.path.join(config.MODEL_DIR, 'fitness_feature_names.pkl'))\n",
        "            common_features = [feature for feature in feature_names if feature in user_df.columns]\n",
        "            if not common_features:\n",
        "                logging.warning(f\"Missing features in user profile for fitness level prediction: {set(feature_names) - set(user_df.columns)}\")\n",
        "                # Handle missing features more robustly, perhaps by raising an error or using a default value strategy\n",
        "                user_df = user_df.reindex(columns=feature_names, fill_value=np.nan)\n",
        "            else:\n",
        "                user_df = user_df[common_features] # Use only available features\n",
        "\n",
        "            processed_data = self.feature_encoders['fitness_level'].transform(user_df)\n",
        "            prediction = self.fitness_level_model.predict(processed_data)[0]\n",
        "            return prediction\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error predicting fitness level: {e}\")\n",
        "            return \"Error\"\n",
        "\n",
        "    def predict_training_plan(self, user_profile: Dict) -> Dict:\n",
        "        if self.training_params_model is None or 'training_params' not in self.feature_encoders:\n",
        "            logging.error(\"Training parameters model or preprocessor not loaded.\")\n",
        "            return {\"error\": \"Model not loaded\"}\n",
        "        try:\n",
        "            user_df = pd.DataFrame([user_profile])\n",
        "            feature_names = joblib.load(os.path.join(config.MODEL_DIR, 'training_params_feature_names.pkl'))\n",
        "            common_features = [feature for feature in feature_names if feature in user_df.columns]\n",
        "            if not common_features:\n",
        "                logging.warning(f\"Missing features in user profile for training plan prediction: {set(feature_names) - set(user_df.columns)}\")\n",
        "                user_df = user_df.reindex(columns=feature_names, fill_value=np.nan)\n",
        "            else:\n",
        "                user_df = user_df[common_features] # Use only available features\n",
        "\n",
        "            processed_data = self.feature_encoders['training_params'].transform(user_df)\n",
        "            prediction = self.training_params_model.predict(processed_data)[0].tolist()\n",
        "\n",
        "            plan = {\"workout_frequency\": config.TRAINING_PLAN_CONFIG.get(\"workout_frequency\", \"3 days per week\"), \"exercises\": []}\n",
        "\n",
        "            for exercise_config in config.TRAINING_PLAN_CONFIG.get(\"exercises\", []):\n",
        "                name = exercise_config.get(\"name\")\n",
        "                sets_index = exercise_config.get(\"sets_index\")\n",
        "                reps_index = exercise_config.get(\"reps_index\")\n",
        "                if name is not None and sets_index is not None and reps_index is not None and sets_index < len(prediction) and reps_index < len(prediction):\n",
        "                    exercise_info = self.exercise_database.get(name, {})\n",
        "                    plan[\"exercises\"].append({\n",
        "                        \"name\": name,\n",
        "                        \"sets\": round(prediction[sets_index]),\n",
        "                        \"repetitions\": round(prediction[reps_index]),\n",
        "                        \"details\": exercise_info.get(\"description\"),\n",
        "                        \"muscle_group\": exercise_info.get(\"muscle_group\")\n",
        "                        # Add more details from exercise_info as needed\n",
        "                    })\n",
        "                elif name:\n",
        "                    logging.warning(f\"Could not generate parameters for {name}.\")\n",
        "\n",
        "            return plan\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error predicting training plan: {e}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def predict_dietary_needs(self, user_profile: Dict) -> Dict:\n",
        "        if self.dietary_needs_model is None or 'dietary_needs' not in self.feature_encoders:\n",
        "            logging.error(\"Dietary needs model or preprocessor not loaded.\")\n",
        "            return {\"error\": \"Model not loaded\"}\n",
        "        try:\n",
        "            user_df = pd.DataFrame([user_profile])\n",
        "            feature_names = joblib.load(os.path.join(config.MODEL_DIR, 'dietary_needs_feature_names.pkl'))\n",
        "            common_features = [feature for feature in feature_names if feature in user_df.columns]\n",
        "            if not common_features:\n",
        "                logging.warning(f\"Missing features in user profile for dietary needs prediction: {set(feature_names) - set(user_df.columns)}\")\n",
        "                user_df = user_df.reindex(columns=feature_names, fill_value=np.nan)\n",
        "            else:\n",
        "                user_df = user_df[common_features] # Use only available features\n",
        "            processed_data = self.feature_encoders['dietary_needs'].transform(user_df)\n",
        "            prediction = self.dietary_needs_model.predict(processed_data)[0].tolist() # Assuming output is [calories, protein, carbs, fat]\n",
        "            needs = {\n",
        "                \"daily_calories\": round(prediction[0]),\n",
        "                \"macronutrient_targets\": {\n",
        "                    \"protein\": round(prediction[1]),\n",
        "                    \"carbs\": round(prediction[2]),\n",
        "                    \"fat\": round(prediction[3])\n",
        "                }\n",
        "            }\n",
        "            return needs\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error predicting dietary needs: {e}\")\n",
        "            return {\"error\": str(e)}"
      ],
      "metadata": {
        "id": "cQ5TcbDVb6NA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Training Function ---\n",
        "def train_models(config: Configuration):\n",
        "    \"\"\"Trains and saves the AI models.\"\"\"\n",
        "    preprocessor = DataPreprocessor()\n",
        "\n",
        "    # --- Train Fitness Level Classification Model ---\n",
        "    try:\n",
        "        # expected_cols_fitness = ['age', 'gender', 'weight', 'height', 'pushups', 'squats', 'weight_lifted_squat_max', 'weight_lifted_bench_max', 'activity_level', 'fitness_level'] # Adjust based on your data\n",
        "        fitness_data = preprocessor.load_data(os.path.join(config.DATA_DIR, config.FITNESS_LEVEL_DATA_FILE)) # Removed expected_cols for example data\n",
        "        fitness_data = preprocessor.preprocess_user_data(fitness_data)\n",
        "        X_fitness, y_fitness = fitness_data.drop('fitness_level', axis=1, errors='ignore'), fitness_data['fitness_level']\n",
        "        X_train_fitness, X_test_fitness, y_train_fitness, y_test_fitness = preprocessor.split_data(fitness_data, 'fitness_level')\n",
        "\n",
        "        fitness_preprocessor = preprocessor.create_preprocessing_pipeline(X_train_fitness)\n",
        "        X_train_processed_fitness = fitness_preprocessor.fit_transform(X_train_fitness)\n",
        "        X_test_processed_fitness = fitness_preprocessor.transform(X_test_fitness)\n",
        "\n",
        "        trainer_fitness = ModelTrainer(model_type='fitness_level')\n",
        "\n",
        "        # --- Hyperparameter Tuning Example (Uncomment and configure to use) ---\n",
        "        from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "        # Define parameter grid for fitness level model\n",
        "        fitness_param_grid = {'n_estimators': [50, 100], 'max_depth': [5, 10, None]}\n",
        "        tuner_fitness = HyperparameterTuner(model_type='fitness_level', model=RandomForestClassifier(random_state=config.RANDOM_STATE), param_grid=fitness_param_grid)\n",
        "        # Assuming you have X_train_processed_fitness and y_train_fitness from the data loading and preprocessing step\n",
        "        tuner_fitness.tune_hyperparameters(X_train_processed_fitness, y_train_fitness, scoring='accuracy')\n",
        "        best_fitness_model = tuner_fitness.get_best_model()\n",
        "        if best_fitness_model:\n",
        "            trainer_fitness.model = best_fitness_model\n",
        "            logging.info(f\"Using best hyperparameters for fitness level: {tuner_fitness.best_model.get_params()}\")\n",
        "\n",
        "        trainer_fitness.train_model(X_train_processed_fitness, y_train_fitness)\n",
        "        trainer_fitness.evaluate_model(X_test_processed_fitness, y_test_fitness)\n",
        "        trainer_fitness.save_model(config.FITNESS_LEVEL_MODEL_NAME)\n",
        "        joblib.dump(fitness_preprocessor, os.path.join(config.MODEL_DIR, 'fitness_preprocessor.pkl'))\n",
        "        joblib.dump(X_train_fitness.columns.tolist(), os.path.join(config.MODEL_DIR, 'fitness_feature_names.pkl'))\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.warning(\"Fitness level training data not found. Skipping training.\")\n",
        "    except ValueError as ve:\n",
        "        logging.error(f\"Data validation error for fitness level model: {ve}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training fitness level model: {e}\")\n",
        "\n",
        "    # --- Train Training Parameters Regression Model ---\n",
        "    try:\n",
        "        target_columns_tp = ['squat_sets', 'squat_reps', 'bench_sets', 'bench_reps', 'deadlift_sets', 'deadlift_reps', 'overhead_sets', 'overhead_reps'] # Adjust based on your data\n",
        "        # expected_cols_tp = ['age', 'gender', 'weight', 'height', 'activity_level'] + target_columns_tp # Add all expected columns\n",
        "        training_params_data = preprocessor.load_data(os.path.join(config.DATA_DIR, config.TRAINING_PARAMS_DATA_FILE)) # Removed expected_cols for example data\n",
        "        training_params_data = preprocessor.preprocess_user_data(training_params_data)\n",
        "        X_training_params, y_training_params = training_params_data.drop(target_columns_tp, axis=1, errors='ignore'), training_params_data[target_columns_tp]\n",
        "        X_train_tp, X_test_tp, y_train_tp, y_test_tp = preprocessor.split_data(training_params_data, target_columns_tp)\n",
        "\n",
        "        training_preprocessor = preprocessor.create_preprocessing_pipeline(X_train_tp)\n",
        "        X_train_processed_tp = training_preprocessor.fit_transform(X_train_tp)\n",
        "        X_test_processed_tp = training_preprocessor.transform(X_test_tp)\n",
        "\n",
        "        trainer_training_params = ModelTrainer(model_type='training_params')\n",
        "        trainer_training_params.train_model(X_train_processed_tp, y_train_tp)\n",
        "        trainer_training_params.evaluate_model(X_test_processed_tp, y_test_tp)\n",
        "        trainer_training_params.save_model(config.TRAINING_PARAMS_MODEL_NAME)\n",
        "        joblib.dump(training_preprocessor, os.path.join(config.MODEL_DIR, 'training_preprocessor.pkl'))\n",
        "        joblib.dump(X_train_tp.columns.tolist(), os.path.join(config.MODEL_DIR, 'training_params_feature_names.pkl'))\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.warning(\"Training parameters data not found. Skipping training.\")\n",
        "    except ValueError as ve:\n",
        "        logging.error(f\"Data validation error for training parameters model: {ve}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training training parameters model: {e}\")\n",
        "\n",
        "    # --- Train Dietary Needs Regression Model ---\n",
        "    try:\n",
        "        target_columns_dn = ['calories', 'protein', 'carbs', 'fat']\n",
        "        # expected_cols_dn = ['age', 'gender', 'weight', 'height', 'activity_level'] + target_columns_dn\n",
        "        dietary_needs_data = preprocessor.load_data(os.path.join(config.DATA_DIR, config.DIETARY_NEEDS_DATA_FILE)) # Removed expected_cols for example data\n",
        "        dietary_needs_data = preprocessor.preprocess_user_data(dietary_needs_data)\n",
        "        X_dietary_needs, y_dietary_needs = dietary_needs_data.drop(target_columns_dn, axis=1, errors='ignore'), dietary_needs_data[target_columns_dn]\n",
        "        X_train_dn, X_test_dn, y_train_dn, y_test_dn = preprocessor.split_data(dietary_needs_data, target_columns_dn)\n",
        "\n",
        "        dietary_preprocessor = preprocessor.create_preprocessing_pipeline(X_train_dn)\n",
        "        X_train_processed_dn = dietary_preprocessor.fit_transform(X_train_dn)\n",
        "        X_test_processed_dn = dietary_preprocessor.transform(X_test_dn)\n",
        "\n",
        "        trainer_dietary_needs = ModelTrainer(model_type='dietary_needs')\n",
        "        trainer_dietary_needs.train_model(X_train_processed_dn, y_train_dn)\n",
        "        trainer_dietary_needs.evaluate_model(X_test_processed_dn, y_test_dn)\n",
        "        trainer_dietary_needs.save_model(config.DIETARY_NEEDS_MODEL_NAME)\n",
        "        joblib.dump(dietary_preprocessor, os.path.join(config.MODEL_DIR, 'dietary_preprocessor.pkl'))\n",
        "        joblib.dump(X_train_dn.columns.tolist(), os.path.join(config.MODEL_DIR, 'dietary_needs_feature_names.pkl'))\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.warning(\"Dietary needs training data not found. Skipping training.\")\n",
        "    except ValueError as ve:\n",
        "        logging.error(f\"Data validation error for dietary needs model: {ve}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error training dietary needs model: {e}\")"
      ],
      "metadata": {
        "id": "kN9PdhupcDTI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 9. TensorFlow Lite Conversion Function ---\n",
        "def convert_sklearn_model_to_tflite(model_path: str, output_path: str, input_shape: Tuple):\n",
        "    \"\"\"Converts a trained scikit-learn model to TensorFlow Lite format.\"\"\"\n",
        "    try:\n",
        "        # Load the scikit-learn model\n",
        "        sklearn_model = joblib.load(model_path)\n",
        "\n",
        "        # Define the input specification for ONNX conversion\n",
        "        initial_type = [('float_input', FloatTensorType(input_shape))]\n",
        "\n",
        "        # Convert the scikit-learn model to ONNX format\n",
        "        onnx_model = convert_sklearn(sklearn_model, initial_types=initial_type)\n",
        "        onnx_file_path = output_path.replace(\".tflite\", \".onnx\")\n",
        "        with open(onnx_file_path, \"wb\") as f:\n",
        "            f.write(onnx_model.SerializeToString())\n",
        "        logging.info(f\"ONNX model saved to: {onnx_file_path}\")\n",
        "\n",
        "        # Convert the ONNX model to TensorFlow Lite\n",
        "        # Use tf.lite.TFLiteConverter.from_onnx if you have a newer TensorFlow version\n",
        "        try:\n",
        "            converter = tf.lite.TFLiteConverter.from_onnx(onnx_file_path)\n",
        "        except AttributeError:\n",
        "            converter = tf.compat.v1.lite.TFLiteConverter.from_onnx_model(onnx_model)\n",
        "        tflite_model = converter.convert()\n",
        "\n",
        "        # Save the TensorFlow Lite model\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "        logging.info(f\"TensorFlow Lite model saved to: {output_path}\")\n",
        "        os.remove(onnx_file_path) # Clean up the ONNX file\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logging.error(f\"Model not found at: {model_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error converting {model_path} to TensorFlow Lite: {e}\")\n",
        "\n",
        "def convert_sklearn_to_tflite(config: Configuration):\n",
        "    \"\"\"Converts trained scikit-learn models to TensorFlow Lite format.\"\"\"\n",
        "    # Ensure TensorFlow version is compatible with ONNX conversion (>= 2.7 recommended)\n",
        "    try:\n",
        "        import tensorflow as tf\n",
        "        if tf.__version__ < '2.7':\n",
        "            logging.warning(f\"TensorFlow version {tf.__version__} might be older than recommended for ONNX to TFLite conversion. Consider upgrading to 2.7 or higher.\")\n",
        "    except ImportError:\n",
        "        logging.error(\"TensorFlow not found.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        fitness_feature_count = joblib.load(os.path.join(config.MODEL_DIR, 'fitness_feature_names.pkl')).__len__()\n",
        "        convert_sklearn_model_to_tflite(\n",
        "            os.path.join(config.MODEL_DIR, config.FITNESS_LEVEL_MODEL_NAME),\n",
        "            os.path.join(config.TFLITE_MODEL_DIR, 'fitness_level_model.tflite'),\n",
        "            (None, fitness_feature_count)\n",
        "        )\n",
        "    except FileNotFoundError:\n",
        "        logging.warning(\"Fitness level feature names not found, skipping TFLite conversion for this model.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during fitness level TFLite conversion: {e}\")\n",
        "\n",
        "    try:\n",
        "        training_feature_count = joblib.load(os.path.join(config.MODEL_DIR, 'training_params_feature_names.pkl')).__len__()\n",
        "        convert_sklearn_model_to_tflite(\n",
        "            os.path.join(config.MODEL_DIR, config.TRAINING_PARAMS_MODEL_NAME),\n",
        "            os.path.join(config.TFLITE_MODEL_DIR, 'training_params_model.tflite'),\n",
        "            (None, training_feature_count)\n",
        "        )\n",
        "    except FileNotFoundError:\n",
        "        logging.warning(\"Training parameters feature names not found, skipping TFLite conversion for this model.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during training parameters TFLite conversion: {e}\")\n",
        "\n",
        "    try:\n",
        "        dietary_feature_count = joblib.load(os.path.join(config.MODEL_DIR, 'dietary_needs_feature_names.pkl')).__len__()\n",
        "        convert_sklearn_model_to_tflite(\n",
        "            os.path.join(config.MODEL_DIR, config.DIETARY_NEEDS_MODEL_NAME),\n",
        "            os.path.join(config.TFLITE_MODEL_DIR, 'dietary_needs_model.tflite'),\n",
        "            (None, dietary_feature_count)\n",
        "        )\n",
        "    except FileNotFoundError:\n",
        "        logging.warning(\"Dietary needs feature names not found, skipping TFLite conversion for this model.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during dietary needs TFLite conversion: {e}\")"
      ],
      "metadata": {
        "id": "WbSISHcVcJVA"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 10. Main Function to Run Predictions ---\n",
        "def predictIt(user_profile):\n",
        "    \"\"\"Loads trained models and demonstrates prediction for a sample user.\"\"\"\n",
        "    prediction_engine = PredictionEngine()\n",
        "    preprocessor = DataPreprocessor() # Instantiate DataPreprocessor here\n",
        "    try:\n",
        "        prediction_engine.load_models()\n",
        "        # Preprocess the sample user profile to calculate 'bmi'\n",
        "        sample_user_profile_df = pd.DataFrame([user_profile])\n",
        "        processed_user_profile_df = preprocessor.preprocess_user_data(sample_user_profile_df)\n",
        "        processed_user_profile = processed_user_profile_df.iloc[0].to_dict()\n",
        "\n",
        "        fitness_level = prediction_engine.predict_fitness_level(processed_user_profile)\n",
        "        print(f\"\\nPredicted Fitness Level: {fitness_level}\")\n",
        "\n",
        "        training_plan = prediction_engine.predict_training_plan(processed_user_profile)\n",
        "        print(\"\\n--- Predicted Training Plan ---\")\n",
        "        print(training_plan)\n",
        "\n",
        "        dietary_needs = prediction_engine.predict_dietary_needs(processed_user_profile)\n",
        "        print(\"\\n--- Predicted Dietary Needs ---\")\n",
        "        print(dietary_needs)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during prediction: {e}\")"
      ],
      "metadata": {
        "id": "NnzK4IsKcMbJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 11. Conceptual Deployment (Illustrative) ---\n",
        "def deploy_model_conceptual(config: Configuration):\n",
        "    \"\"\"Conceptual model deployment function.\"\"\"\n",
        "    logging.info(\"Conceptual model deployment started...\")\n",
        "    logging.info(f\"Trained scikit-learn models are saved in: {config.MODEL_DIR}\")\n",
        "    logging.info(f\"TensorFlow Lite models are saved in: {config.TFLITE_MODEL_DIR}\")\n",
        "    logging.info(\"Conceptual model deployment finished.\")"
      ],
      "metadata": {
        "id": "8HvycG53cQqQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example user profile (replace with actual user input)\n",
        "sample_user_profile = {\n",
        "    \"age\": 29,\n",
        "    \"gender\": \"male\",\n",
        "    \"weight\": 88,     # kg\n",
        "    \"height\": 171,    # cm\n",
        "    \"pushups\": 50,\n",
        "    \"squats\": 50,\n",
        "    \"weight_lifted_squat_max\": 40.0,\n",
        "    'weight_lifted_bench':20,\n",
        "    \"weight_lifted_bench_max\": 25.0, # Corrected the missing feature\n",
        "    \"activity_level\": \"Moderately Active\",\n",
        "    \"target_goal\": \"strength\", # Example for dietary needs and training parameters\n",
        "    \"dietary_preference\": \"none\", # Example for dietary needs\n",
        "    # Add other features based on your training data. Ensure these match the features used during training.\n",
        "    \"plank_duration\": 60,\n",
        "    \"running_endurance\": 17,\n",
        "    \"weight_lifted_squat\": 30,\n",
        "}"
      ],
      "metadata": {
        "id": "Nt-_vrpqegXn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24yccqevaAj7",
        "outputId": "b0e17195-ba02-4ce3-adf5-11277c1c1d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.19.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n",
            "WARNING:root:TensorFlow version 2.19.0 might be older than recommended for ONNX to TFLite conversion. Consider upgrading to 2.7 or higher.\n",
            "ERROR:root:Error converting /content/drive/MyDrive/Projects/Get_Fit_App/Ai_Model/models/fitness_level_model.pkl to TensorFlow Lite: type object 'TFLiteConverter' has no attribute 'from_onnx_model'\n",
            "ERROR:root:Error converting /content/drive/MyDrive/Projects/Get_Fit_App/Ai_Model/models/training_params_model.pkl to TensorFlow Lite: type object 'TFLiteConverter' has no attribute 'from_onnx_model'\n",
            "ERROR:root:Error converting /content/drive/MyDrive/Projects/Get_Fit_App/Ai_Model/models/dietary_needs_model.pkl to TensorFlow Lite: type object 'TFLiteConverter' has no attribute 'from_onnx_model'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted Fitness Level: Beginner\n",
            "\n",
            "--- Predicted Training Plan ---\n",
            "{'workout_frequency': '3 days per week', 'exercises': [{'name': 'Barbell Squat', 'sets': 3, 'repetitions': 8, 'details': None, 'muscle_group': None}, {'name': 'Bench Press', 'sets': 3, 'repetitions': 8, 'details': None, 'muscle_group': None}, {'name': 'Deadlift', 'sets': 3, 'repetitions': 6, 'details': None, 'muscle_group': None}, {'name': 'Overhead Press', 'sets': 3, 'repetitions': 7, 'details': None, 'muscle_group': None}]}\n",
            "\n",
            "--- Predicted Dietary Needs ---\n",
            "{'daily_calories': 3938, 'macronutrient_targets': {'protein': 197, 'carbs': 394, 'fat': 131}}\n"
          ]
        }
      ],
      "source": [
        "# --- 12. Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "    # To train the models and convert them to TensorFlow Lite, uncomment the following lines:\n",
        "    train_models(config)\n",
        "    convert_sklearn_to_tflite(config)\n",
        "\n",
        "    # To run predictions using the trained scikit-learn models:\n",
        "    predictIt(sample_user_profile)\n",
        "\n",
        "    # To simulate deployment (conceptual):\n",
        "    deploy_model_conceptual(config)"
      ]
    }
  ]
}